{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender Systems using PySpark\n",
    "=============\n",
    "Recommender Systems have derived great importance in understanding user behavior. It has found its extensive application in e-commerce, user modelling,etc. PySpark is one of the best tools for implementing recommender systems for date of size  upto 5GB. This is because of extremely fast computations in pyspark through its in-memory processing capabilities.\n",
    "> Never update the PATH env variable in \\/etc\\/environment directly the way you modify other variables. Rather add the expanded path in the exisiting PATH env variable declaration as bellow\n",
    "\n",
    "What is Map-Reduce Operation?\n",
    "-----\n",
    "MapReduce consists of three Phases\n",
    "1. Mapping - Lines to (key,value)\n",
    "2. Shuffle and Sorting - (key,value) goes to unique reducer \n",
    "4. Reduction - Processing of data corrosponding to that key\n",
    "For more information look into [http://www.tutorialspoint.com/map_reduce/map_reduce_introduction.htm]\n",
    "\n",
    "Hadoop stores the output of mapper into HDFS datanode at the end of Mapping phase, which consumes a lot of time. Even for shuffle phase, it take some time, because each reducer machine's HDFS is supposed to consists of all the data with one or group of keys atomically. What I mean by Atomic is that you han't have data corrosponding to  a single key stored in different reducer machines. Finally the reading of reducer input and dumping it to final location again takes a toll, because of latency introduced by IO operations.\n",
    "These all problems, completely go away by use of Spark as it does in-memory processing using RDD's and tries to minimize disk-access. It has only one time IO access, which is during sort and shuffle phase.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Map Reduce word count Diagram](MapReduceWordCountOverview1.png \"Word Count - A map-reduce example\")\n",
    "\n",
    "What is HDFS File System and how does a program execute on a cluster?\n",
    "-----\n",
    "HDFS File system stands for Hadoop's Distributed File System. It is used to manage data stored in distributed manner across various machines. Essentially, lets say I have 50 PC or 50 EC2 Machines in Hadoop Cluster. Each of them have lets say 1 TB Storage capacity and 4/8 GB Ram + 2.5GHz Processor. Now we want to use all of these resources that is what we call Distributed Systems.\n",
    "Now I have 10 TB of data, which needs to be processed, a single machine can anyway not store it. So I need to store it in all these machines, and handle it. HDFS file systems helps you do the same. It creates a virtual File system called HDFS. It has a master or namenode which consists of metadata of the  storage on different machines, and abstracts it like a single large unit. The actual data resides on individual machines (50 PC or AWS Virtual machines). These machines store data as datanode. Now HDFS, cleverly integrates these storages, to make a giant virtual storage. That is why it is so popular in Big Data Community.\n",
    "\n",
    "\n",
    "\n",
    "Functional Programming\n",
    "============\n",
    "A programming paradigm is called purely functional if changes introduced by a function are only based on input and only changes/intends to change the output. Like setting of global variable in function is Non-functional, etc. In other programming paradigms, functions are consider second class datatypes, but in functional programming, functions are treated par to par with any datatype. A function can be passed to another function as input, etc. Some examples are Javascript, Scala, Lambda functions in python.\n",
    "Development of Spark was heavily inspired by Scala, and hence the API and structure derives quite a lot of similarites with Scala.\n",
    "\n",
    "Features of Spark\n",
    "============\n",
    "Spark has 6 basic properties which makes it robust\n",
    "\n",
    "1. Immutability\n",
    "2. Lazy execution and Lineage support\n",
    "3. Cacheable\n",
    "4. Type Infered\n",
    "5. Compile Time Safe\n",
    "6. Functional in nature\n",
    "\n",
    "It has 3 basic ways  of treating data : \n",
    "1. RDD - Basic building blocks\n",
    "2. Dataframes - For Optimized query like execution\n",
    "3. Datasets - Dataframes + type-support\n",
    "\n",
    "We will only look into what is RDD to get a context for the latter topics in Tutorial.Full form of RDD is Resilient Distributed Datasets. As the name suggests, it is immutable dataset form, which can behave like single unit while residing in different systems. HDFS is used commonly as distributed storage form for RDD's. However the processing happens in-memory and only uses HDFS in case of insuffient storage in memory for computations.\n",
    "Lazy execution means, It only executes when there a store or print command, or in other ways whenever output is requested. A good example is how we prepare for exams. The syllabus keeps piling up, but we start studying only 24 hrs prior to exams.\n",
    "\n",
    "\n",
    "Basics of pyspark\n",
    "======\n",
    "Pyspark is a python wrapper over spark. The important thing is to understand how it works with sparkcontext. Let us look at a few examples to understand it in brief.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Look into the bellow link for more examples \n",
    "[https://github.com/mahmoudparsian/pyspark-tutorial]\n",
    "\n",
    "\n",
    "Recommender Systems\n",
    "======\n",
    "Today, ECommerce Industry has heavy demand of understanding user behaviour. It employs various methods, starting from bootstapping in knowledge graph, to nlp for understanding the same. Recommender System is a specific application for the same. \n",
    "![Recomender System ](recommender1.png \"Recommender System\")\n",
    "Almost all Recommender Systems consist of sparse storage of User Item Matrix. The example we have considered for tutorial is predicting movie rating for a user who has not yet watched it. Here the item is movie.\n",
    "You can use user-user or item-item similarity as such. which uses cosine similarity for knowing thee correlation between two items.\n",
    "![User Item Matrix](movies.png \"User Item Matrix\")\n",
    "In the above matrix, we need to have two elements which are unknown. The way to find it, is to factorize the matrix into UV, such the the multiplication of U and V results in matrix which is nearest possible to the original matrix. The number of matrix in this optimization problem is same as number of known elements in matrix. We can also combine these constraints as shown in the figure bellow.\n",
    "\n",
    "We are going to use Alternating Least Squares solution for Recommend the unlabelled elements in the matrix.\n",
    "![Alternating Least Square - Matrix Factorization ](matrix_factorization.png \"Matrix Factorization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Recommender in Pyspark\n",
    "======\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using ALS algorithm and hence we have imported it first. The next step is to read the date from its source into RDD. This is done via spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "#All Data\n",
    "ratings_raw_data = sc.textFile('ml-latest-small/ratings.csv')\n",
    "movies_raw_data= sc.textFile('ml-latest-small/movies.csv')\n",
    "#Headers\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "movies_raw_data_header = movies_raw_data.take(1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe the content of movies.csv befor we start getting recommendations for a new user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r\n",
      "\n",
      "1,16,4.0,1217897793\r\n",
      "\n",
      "1,24,1.5,1217895807\r\n",
      "\n",
      "1,32,4.0,1217896246\r\n",
      "\n",
      "1,47,4.0,1217896556\r\n",
      "\n",
      "1,50,4.0,1217896523\r\n",
      "\n",
      "1,110,4.0,1217896150\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,lines in enumerate(open('ml-latest-small/ratings.csv','r').readlines()):\n",
    "    print lines\n",
    "    if(idx>5):break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId,title,genres\r\n",
      "\n",
      "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\r\n",
      "\n",
      "2,Jumanji (1995),Adventure|Children|Fantasy\r\n",
      "\n",
      "3,Grumpier Old Men (1995),Comedy|Romance\r\n",
      "\n",
      "4,Waiting to Exhale (1995),Comedy|Drama|Romance\r\n",
      "\n",
      "5,Father of the Bride Part II (1995),Comedy\r\n",
      "\n",
      "6,Heat (1995),Action|Crime|Thriller\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,lines in enumerate(open('ml-latest-small/movies.csv','r').readlines()):\n",
    "    print lines\n",
    "    if(idx>5):break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'userId', u'movieId', u'rating']\n",
      "(u'1', u'16', u'4.0')\n",
      "(u'1', u'24', u'1.5')\n",
      "(u'1', u'32', u'4.0')\n",
      "(u'1', u'47', u'4.0')\n",
      "(u'1', u'50', u'4.0')\n"
     ]
    }
   ],
   "source": [
    "ratings_data = ratings_raw_data.filter(lambda line: line!=ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()\n",
    "#Print Ratings RDD Data\n",
    "print ratings_raw_data_header.split(',')[0:-1]\n",
    "for elem in ratings_data.take(5):print elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'movieId', u'title']\n",
      "(u'1', u'Toy Story (1995)')\n",
      "(u'2', u'Jumanji (1995)')\n",
      "(u'3', u'Grumpier Old Men (1995)')\n",
      "(u'4', u'Waiting to Exhale (1995)')\n",
      "(u'5', u'Father of the Bride Part II (1995)')\n"
     ]
    }
   ],
   "source": [
    "movies_data = movies_raw_data.filter(lambda line: line!=movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
    "#Print Movies RDD Data\n",
    "print movies_raw_data_header.split(',')[0:-1]\n",
    "for elem in movies_data.take(5): print elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Splitting the data in Train,Validation and Test sets in the done via randomSplit function in pyspark.`\n",
    "> Note that the operations on RDD are functional in nature. In other words, you have lambda functions which are evaluating expressions on object state, rather than introducing any unknown effects. A programming paradigm is called purely functional if changes introduced by a function are only based on input and only changes/intends to change the output. Like setting of global variable in function is Non-functional, etc. Read this blog about functional programming : http://blog.jenkster.com/2015/12/what-is-functional-programming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Set is :  84132\n",
      "Length of Validation Set is :  21207\n"
     ]
    }
   ],
   "source": [
    "training_RDD, validation_RDD = ratings_data.randomSplit([8, 2], seed=0L)\n",
    "print 'Length of Train Set is : ',len(training_RDD.collect())\n",
    "print 'Length of Validation Set is : ',len(validation_RDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'userId', u'movieId', u'rating']\n",
      "(u'1', u'16')\n",
      "(u'1', u'150')\n",
      "(u'1', u'161')\n",
      "(u'1', u'165')\n",
      "(u'1', u'204')\n"
     ]
    }
   ],
   "source": [
    "validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\n",
    "#Printing a subset of modified data\n",
    "print ratings_raw_data_header.split(',')[0:-1]\n",
    "for elem in validation_for_predict_RDD.take(5): print elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rank 4 the RMSE is 0.893634858349\n",
      "For rank 8 the RMSE is 0.896745287132\n",
      "For rank 12 the RMSE is 0.901753642561\n",
      "The best model was trained with rank 4\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "seed = 5L\n",
    "iterations = 10\n",
    "regularization_parameter = 0.1\n",
    "ranks = [4, 8, 12]\n",
    "errors = [0, 0, 0]\n",
    "err = 0\n",
    "tolerance = 0.02\n",
    "\n",
    "min_error = float('inf')\n",
    "best_rank = -1\n",
    "best_iteration = -1\n",
    "for rank in ranks:\n",
    "    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n",
    "                      lambda_=regularization_parameter)\n",
    "    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "    rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n",
    "    error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n",
    "    errors[err] = error\n",
    "    err += 1\n",
    "    print 'For rank %s the RMSE is %s' % (rank, error)\n",
    "    if error < min_error:\n",
    "        min_error = error\n",
    "        best_rank = rank\n",
    "\n",
    "print 'The best model was trained with rank %s' % best_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(userId,movieId,rating)\n",
      "((568, 1084), 3.9166594902518033)\n",
      "((188, 1084), 4.216447131934498)\n",
      "((33, 1084), 4.302628872278145)\n",
      "((525, 1084), 3.8478979869621077)\n",
      "((109, 1084), 4.464118455087252)\n"
     ]
    }
   ],
   "source": [
    "print '(userId,movieId,rating)'\n",
    "for elem in predictions.take(5): print elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((62, 2412), (1.5, 1.770752830443994))\n",
      "((554, 1380), (3.0, 3.672310492776139))\n",
      "((145, 903), (4.0, 4.269041620122563))\n",
      "((403, 51709), (3.0, 3.507399339351336))\n",
      "((262, 1270), (3.0, 3.8147462594987043))\n"
     ]
    }
   ],
   "source": [
    "actual_rating = validation_RDD.map(lambda x : ((int(x[0]),int(x[1])),float(x[2])))\n",
    "predicted_rating=predictions\n",
    "joined_rating=actual_rating.join(predicted_rating)\n",
    "for elem in joined_rating.take(5):print elem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "--------\n",
    "`The instructions are morphed version of the link bellow. I want to thank codementor for the amazing tutorial on recommender systems via PySpark`\n",
    "https://www.codementor.io/spark/tutorial/building-a-recommender-with-apache-spark-python-example-app-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
