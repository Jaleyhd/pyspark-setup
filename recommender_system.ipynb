{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender Systems using PySpark\n",
    "=============\n",
    "Recommender Systems have derived great importance in understanding user behavior. It has found its extensive application in e-commerce, user modelling,etc. PySpark is one of the best tools for implementing recommender systems for date of size  upto 5GB. This is because of extremely fast computations in pyspark through its in-memory processing capabilities.\n",
    "> Never update the PATH env variable in \\/etc\\/environment directly the way you modify other variables. Rather add the expanded path in the exisiting PATH env variable declaration as bellow\n",
    "\n",
    "What is Map-Reduce Operation?\n",
    "-----\n",
    "\n",
    "\n",
    "![Map Reduce word count Diagram](MapReduceWordCountOverview1.png \"Word Count - A map-reduce example\")\n",
    "\n",
    "What is HDFS File System?\n",
    "-----\n",
    "\n",
    "How does a program execute on a cluster?\n",
    "-----\n",
    "\n",
    "Basics of pyspark\n",
    "======\n",
    "\n",
    "Recommender Systems\n",
    "======\n",
    "\n",
    "Recommender in Pyspark\n",
    "======\n",
    "\n",
    "Import Data\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using ALS algorithm and hence we have imported it first. The next step is to read the date from its source into RDD. This is done via spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "findspark.init()\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "from pyspark.mllib.recommendation import ALS\n",
    "#All Data\n",
    "ratings_raw_data = sc.textFile('ml-latest-small/ratings.csv')\n",
    "movies_raw_data= sc.textFile('ml-latest-small/movies.csv')\n",
    "#Headers\n",
    "ratings_raw_data_header = ratings_raw_data.take(1)[0]\n",
    "movies_raw_data_header = movies_raw_data.take(1)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us observe the content of movies.csv befor we start getting recommendations for a new user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\r\n",
      "\n",
      "1,16,4.0,1217897793\r\n",
      "\n",
      "1,24,1.5,1217895807\r\n",
      "\n",
      "1,32,4.0,1217896246\r\n",
      "\n",
      "1,47,4.0,1217896556\r\n",
      "\n",
      "1,50,4.0,1217896523\r\n",
      "\n",
      "1,110,4.0,1217896150\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,lines in enumerate(open('ml-latest-small/ratings.csv','r').readlines()):\n",
    "    print lines\n",
    "    if(idx>5):break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movieId,title,genres\r\n",
      "\n",
      "1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy\r\n",
      "\n",
      "2,Jumanji (1995),Adventure|Children|Fantasy\r\n",
      "\n",
      "3,Grumpier Old Men (1995),Comedy|Romance\r\n",
      "\n",
      "4,Waiting to Exhale (1995),Comedy|Drama|Romance\r\n",
      "\n",
      "5,Father of the Bride Part II (1995),Comedy\r\n",
      "\n",
      "6,Heat (1995),Action|Crime|Thriller\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,lines in enumerate(open('ml-latest-small/movies.csv','r').readlines()):\n",
    "    print lines\n",
    "    if(idx>5):break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1', u'16', u'4.0')\n",
      "(u'1', u'24', u'1.5')\n",
      "(u'1', u'32', u'4.0')\n",
      "(u'1', u'47', u'4.0')\n",
      "(u'1', u'50', u'4.0')\n",
      "(u'1', u'110', u'4.0')\n",
      "(u'1', u'150', u'3.0')\n"
     ]
    }
   ],
   "source": [
    "ratings_data = ratings_raw_data.filter(lambda line: line!=ratings_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1],tokens[2])).cache()\n",
    "#Print Ratings RDD Data\n",
    "for idx,elem in enumerate(ratings_data.collect()):\n",
    "    print elem\n",
    "    if(idx>5):break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'1', u'Toy Story (1995)')\n",
      "(u'2', u'Jumanji (1995)')\n",
      "(u'3', u'Grumpier Old Men (1995)')\n",
      "(u'4', u'Waiting to Exhale (1995)')\n",
      "(u'5', u'Father of the Bride Part II (1995)')\n",
      "(u'6', u'Heat (1995)')\n",
      "(u'7', u'Sabrina (1995)')\n"
     ]
    }
   ],
   "source": [
    "movies_data = movies_raw_data.filter(lambda line: line!=movies_raw_data_header)\\\n",
    "    .map(lambda line: line.split(\",\")).map(lambda tokens: (tokens[0],tokens[1])).cache()\n",
    "#Print Movies RDD Data\n",
    "for idx,elem in enumerate(movies_data.collect()):\n",
    "    print elem\n",
    "    if(idx>5):break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Splitting the data in Train,Validation and Test sets in the done via randomSplit function in pyspark.`\n",
    "> Note that the operations on RDD are functional in nature. In other words, you have lambda functions which are evaluating expressions on object state, rather than introducing any unknown effects. A programming paradigm is called purely functional if changes introduced by a function are only based on input and only changes/intends to change the output. Read this blog about functional programming : http://blog.jenkster.com/2015/12/what-is-functional-programming.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Set is :  63449\n",
      "Length of Validation Set is :  20683\n",
      "Length of Testing Set is :  21207\n"
     ]
    }
   ],
   "source": [
    "training_RDD, validation_RDD, test_RDD = small_ratings_data.randomSplit([6, 2, 2], seed=0L)\n",
    "print 'Length of Train Set is : ',len(training_RDD.collect())\n",
    "print 'Length of Validation Set is : ',len(validation_RDD.collect())\n",
    "print 'Length of Testing Set is : ',len(test_RDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "--------\n",
    "\n",
    "https://www.codementor.io/spark/tutorial/building-a-recommender-with-apache-spark-python-example-app-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
